import streamlit as st
import sys
import os
import json
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ í™•ë³´
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../")))

from src.services import agent
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

from src.core.config import settings

# ==============================================================================
# 0. í—¬í¼ í•¨ìˆ˜: í›„ì† ì§ˆë¬¸ ìƒì„±ê¸°
# ==============================================================================
def generate_followup_questions(last_query, last_answer):
    """
    LLMì„ ì´ìš©í•´ ì‚¬ìš©ìê°€ ë‹¤ìŒì— ë¬¼ì–´ë³¼ ë§Œí•œ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš© (ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´)
        if settings.LLM_PROVIDER == "openai":
            llm = ChatOpenAI(model="gpt-4o-mini", api_key=settings.OPENAI_API_KEY, temperature=0.7)
        else:
            llm = ChatOllama(model=settings.OLLAMA_MODEL, temperature=0.7, base_url=settings.OLLAMA_BASE_URL)

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant suggesting follow-up questions for a cyber security analyst."),
            ("human", f"""
            Based on the user's previous question and the agent's answer, suggest 3 short, relevant follow-up questions in Korean.
            Return ONLY the questions, separated by pipes (|). Do not add numbering or quotes.
            
            [User Question] {last_query}
            [Agent Answer] {last_answer}
            
            Example format: ì´ ì•…ì„±ì½”ë“œì˜ ì¹¨í•´ì§€í‘œ(IOC)ëŠ” ë­ì•¼?|ê´€ë ¨ëœ ëŒ€ì‘ ë°©ì•ˆì€?|ì–´ë–¤ ê·¸ë£¹ì´ ë°°í›„ì•¼?
            """)
        ])
        
        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({})
        return [q.strip() for q in result.split('|') if q.strip()][:3]
    except:
        return [] # ì—ëŸ¬ ë‚˜ë©´ ì¶”ì²œ ì§ˆë¬¸ ì•ˆ ë„ì›€

# ==============================================================================
# 1. í˜ì´ì§€ ì„¤ì •
# ==============================================================================
st.set_page_config(page_title="Cyber Threat Analyst", page_icon="ğŸ•µï¸â€â™‚ï¸", layout="wide")

st.title("ğŸ•µï¸â€â™‚ï¸ Neo4j Cyber Threat Analyst")
st.markdown("""
Neo4j Knowledge Graphë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ì•ˆ ìœ„í˜‘ì„ ë¶„ì„í•˜ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.  
**MITRE ATT&CK, CISA KEV, URLHaus** ë°ì´í„°ë¥¼ êµì°¨ ë¶„ì„í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.
""")

# ==============================================================================
# 2. Session State ì´ˆê¸°í™”
# ==============================================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "langchain_history" not in st.session_state:
    st.session_state.langchain_history = []

# [ë³€ê²½] ì…ë ¥ íŠ¸ë¦¬ê±° ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜
if "trigger_query" not in st.session_state:
    st.session_state.trigger_query = None

# [ì‹ ê·œ] ë§ˆì§€ë§‰ ë‹µë³€ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ì €ì¥
if "followup_suggestions" not in st.session_state:
    st.session_state.followup_suggestions = []

# ==============================================================================
# 3. ì‚¬ì´ë“œë°”: ìƒ˜í”Œ ì§ˆë¬¸
# ==============================================================================
with st.sidebar:
    st.header("ğŸ“ Sample Questions")
    sample_questions = [
        "ì´ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ êµ¬ì¡°ë¥¼ ì•Œë ¤ì¤˜.",
        "ìµœê·¼ 'MongoDB'ì™€ ê´€ë ¨ëœ ì·¨ì•½ì (CVE)ì´ ìˆì–´?",
        "Mozi ë´‡ë„·ê³¼ ê´€ë ¨ëœ ì•…ì„± URL 5ê°œë§Œ ì°¾ì•„ì¤˜.",
        "CVE-2025-14733 ì·¨ì•½ì ì€ ì–´ë–¤ ê³µê²© ê¸°ë²•ì´ë‘ ì—°ê´€ë¼?",
        "APT29 ê·¸ë£¹ì´ ì‚¬ìš©í•˜ëŠ” ì•…ì„±ì½”ë“œë“¤ì€ ë­ì•¼?",
        "IP '1.2.3.4'ë‚˜ í•´ì‹œê°’ ê°™ì€ ì•„í‹°íŒ©íŠ¸ë“¤ ê°„ì˜ ìˆ¨ê²¨ì§„ ì—°ê´€ì„±ì„ ë¶„ì„í•´ì¤˜. (í…ŒìŠ¤íŠ¸ìš©)",
    ]

    for q in sample_questions:
        if st.button(q, use_container_width=True):
            st.session_state.trigger_query = q
            st.session_state.followup_suggestions = [] # ìƒˆ ì§ˆë¬¸ì´ë¯€ë¡œ ê¸°ì¡´ ì¶”ì²œ ì´ˆê¸°í™”
            st.rerun()

# ==============================================================================
# 4. ë©”ì¸ ë¡œì§ í•¨ìˆ˜
# ==============================================================================
def process_query(user_input):
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ UI í‘œì‹œ ë° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # 2. ì—ì´ì „íŠ¸ ì‹¤í–‰
    final_response = ""
    
    # UIì— ê·¸ë¦¬ê¸° (ì´ì „ ë©”ì‹œì§€ë“¤ì€ ì•„ë˜ ë©”ì¸ ë£¨í”„ì—ì„œ ì´ë¯¸ ê·¸ë ¤ì§)
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        status_placeholder = st.status("ğŸ§  Agent is reasoning...", expanded=True)
        
        try:
            graph = agent.build_agent_graph()
            current_human_msg = HumanMessage(content=user_input)
            input_messages = st.session_state.langchain_history + [current_human_msg]
            
            step_count = 0
            
            with status_placeholder:
                for event in graph.stream({"messages": input_messages}, stream_mode="values"):
                    current_state_messages = event["messages"]
                    if not current_state_messages: continue
                    
                    last_msg = current_state_messages[-1]
                    
                    if isinstance(last_msg, AIMessage) and last_msg.tool_calls:
                        for tc in last_msg.tool_calls:
                            step_count += 1
                            st.write(f"**Step {step_count}:** ğŸ¤” Decided to use tool `{tc['name']}`")
                            with st.expander(f"Arguments for {tc['name']}", expanded=False):
                                st.code(json.dumps(tc['args'], indent=2), language="json")

                    elif isinstance(last_msg, ToolMessage):
                        st.write(f"**Step {step_count}:** ğŸ” Tool Output (`{last_msg.name}`)")
                        with st.expander("Show Result", expanded=False):
                            try:
                                content_json = json.loads(last_msg.content)
                                st.json(content_json)
                            except:
                                st.code(last_msg.content[:1000] + "...", language="text") # ë„ˆë¬´ ê¸¸ë©´ ìë¦„

                    elif isinstance(last_msg, AIMessage) and last_msg.content:
                        if not last_msg.tool_calls:
                            final_response = last_msg.content
            
            status_placeholder.update(label="âœ… Analysis Complete", state="complete", expanded=False)
            
            if final_response:
                response_placeholder.markdown(final_response)
                
                # ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": final_response})
                st.session_state.langchain_history.append(current_human_msg)
                st.session_state.langchain_history.append(AIMessage(content=final_response))
                
                # [ì‹ ê·œ] í›„ì† ì§ˆë¬¸ ìƒì„± (ë¹„ë™ê¸°ì²˜ëŸ¼ ë³´ì´ê²Œ ì²˜ë¦¬)
                suggestions = generate_followup_questions(user_input, final_response)
                st.session_state.followup_suggestions = suggestions
                st.rerun() # ì¶”ì²œ ì§ˆë¬¸ ë Œë”ë§ì„ ìœ„í•´ ë¦¬ëŸ°
                
            else:
                response_placeholder.error("ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            status_placeholder.update(label="âŒ Error Occurred", state="error")
            st.error(f"Error: {e}")

# ==============================================================================
# 5. í™”ë©´ ë Œë”ë§ ë£¨í”„
# ==============================================================================

# A. ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ 
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# B. í›„ì† ì§ˆë¬¸ ì„ íƒì§€ ì¶œë ¥ (ë§ˆì§€ë§‰ì´ AI ë‹µë³€ì¼ ë•Œë§Œ)
if st.session_state.messages and st.session_state.messages[-1]["role"] == "assistant":
    if st.session_state.followup_suggestions:
        st.write("ğŸ‘‰ **Suggested Questions:**")
        cols = st.columns(len(st.session_state.followup_suggestions))
        for idx, suggestion in enumerate(st.session_state.followup_suggestions):
            if cols[idx].button(suggestion, key=f"suggest_{len(st.session_state.messages)}_{idx}"):
                st.session_state.trigger_query = suggestion
                st.session_state.followup_suggestions = [] # ì„ íƒí–ˆìœ¼ë‹ˆ ì´ˆê¸°í™”
                st.rerun()

# C. íŠ¸ë¦¬ê±° í™•ì¸ (ì‚¬ì´ë“œë°” ë˜ëŠ” ì¶”ì²œ ì§ˆë¬¸ í´ë¦­ ì‹œ)
if st.session_state.trigger_query:
    query = st.session_state.trigger_query
    st.session_state.trigger_query = None # ì†Œë¹„
    process_query(query)

# D. ì±„íŒ… ì…ë ¥ì°½ (í•­ìƒ ìµœí•˜ë‹¨ì— ìœ ì§€ë¨)
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.followup_suggestions = [] # ìƒˆ ì§ˆë¬¸ ì…ë ¥ ì‹œ ì¶”ì²œ ì´ˆê¸°í™”
    process_query(prompt)