import os
import sys
import json
import re
import warnings
from difflib import SequenceMatcher
from typing import List, Optional, Literal
from pydantic import BaseModel, Field

warnings.filterwarnings("ignore")

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from src.core.config import settings
from src.core.graph_client import graph_client

# ==============================================================================
# 1. Pydantic ìŠ¤í‚¤ë§ˆ
# ==============================================================================
class Entity(BaseModel):
    name: str = Field(description="Specific name. Remove defanging brackets (e.g. 1.1[.]1.1 -> 1.1.1.1)")
    label: Literal[
        "Incident", "Malware", "ThreatGroup", "Vulnerability", 
        "AttackTechnique", "Indicator", "SecurityEntity", "Tool"
    ] = Field(description="Detailed type.")
    reasoning: str = Field(description="Short reasoning.")
    
    # ë‚´ë¶€ ì²˜ë¦¬ìš©
    normalized_name: Optional[str] = None
    existing_id: Optional[str] = None
    db_label: Optional[str] = None 
    match_score: float = 0.0

class Relationship(BaseModel):
    source: str
    target: str
    type: str

class GraphExtraction(BaseModel):
    entities: List[Entity]
    relationships: List[Relationship]

# ==============================================================================
# 2. Regex ê¸°ë°˜ ê°•ì œ ì¶”ì¶œê¸° (LLM ë³´ì™„ìš©)
# ==============================================================================
def extract_iocs_regex(text: str) -> List[Entity]:
    """
    LLMì´ ë†“ì¹œ IOC(IP, URL, MD5 ë“±)ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê°•ì œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    iocs = []
    
    # 1. IPv4 (Defanged í¬í•¨: 1.1.1[.]1)
    ip_pattern = r'\b(?:\d{1,3}(?:\[?\.\]?|\(\.\))\d{1,3}(?:\[?\.\]?|\(\.\))\d{1,3}(?:\[?\.\]?|\(\.\))\d{1,3})\b'
    # 2. MD5 (32 hex chars)
    md5_pattern = r'\b[a-fA-F0-9]{32}\b'
    # 3. URL (hxxp, http[:] ë“± í¬í•¨)
    url_pattern = r'(?:hxxp|http|https)(?:\[?:\s*\]?|:)(?:/{2}|\\{2})(?:[a-zA-Z0-9\-\.]+(?:\[?\.\]?)[a-zA-Z]{2,})(?:[^\s]*)'
    # 4. Domain (aaaa[.]cyou)
    domain_pattern = r'\b(?:[a-zA-Z0-9\-]+\.)+(?:\[?\.\]?)[a-zA-Z]{2,}\b'
    # 5. CVE
    cve_pattern = r'CVE-\d{4}-\d{4,7}'

    # CVE
    for match in re.findall(cve_pattern, text, re.IGNORECASE):
        iocs.append(Entity(name=match, label="Vulnerability", reasoning="Regex Extracted CVE"))

    # IPs
    for match in re.findall(ip_pattern, text):
        # CVEë‚˜ ë²„ì „ ë²ˆí˜¸(2024.12.31) ì˜¤íƒì§€ ì œì™¸
        if not re.search(r'^\d{4}', match): 
            iocs.append(Entity(name=match, label="Indicator", reasoning="Regex Extracted IP"))

    # MD5
    for match in re.findall(md5_pattern, text):
        iocs.append(Entity(name=match, label="Indicator", reasoning="Regex Extracted MD5"))
        
    # URLs
    for match in re.findall(url_pattern, text, re.IGNORECASE):
        iocs.append(Entity(name=match, label="Indicator", reasoning="Regex Extracted URL"))

    # Domains (URLì— í¬í•¨ ì•ˆëœ ê²ƒë“¤)
    for match in re.findall(domain_pattern, text, re.IGNORECASE):
        # ì œì™¸ í‚¤ì›Œë“œ
        if any(x in match.lower() for x in ["ahnlab", "security", "korea"]): continue
        iocs.append(Entity(name=match, label="Indicator", reasoning="Regex Extracted Domain"))

    return iocs

# ==============================================================================
# 3. ìœ í‹¸ë¦¬í‹° & ì •ì œ
# ==============================================================================
def get_extractor():
    if settings.LLM_PROVIDER == "openai":
        # ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ë¥¼ ìœ„í•´ ëª¨ë¸ ì§€ì • ì¤‘ìš” (GPT-4o ê¶Œìž¥)
        llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0)
    else:
        llm = ChatOllama(model=settings.OLLAMA_MODEL, temperature=0)
    return llm.with_structured_output(GraphExtraction)

def clean_indicator(text: str) -> str:
    """[.] ì œê±° ë° hxxp ë³€í™˜, í¬íŠ¸ ë¶„ë¦¬ ì „ì²˜ë¦¬"""
    text = text.replace("[.]", ".").replace("(.)", ".")
    text = text.replace("[:]", ":").replace("http", "http").replace("hxxp", "http")
    return text

def split_composite_indicator(entity: Entity) -> List[Entity]:
    """IP:Port ë¶„ë¦¬ ë¡œì§"""
    if entity.label != "Indicator": return [entity]
    
    # ì •ê·œì‹: IP:Port (ì—¬ëŸ¬ê°œ ê°€ëŠ¥)
    match = re.match(r"^(\d{1,3}(?:\.\d{1,3}){3}):([\d,]+)$", entity.name)
    if match:
        ip = match.group(1)
        ports = match.group(2).split(',')
        new_entities = [Entity(name=ip, label="Indicator", reasoning="Extracted IP")]
        for p in ports:
            new_entities.append(Entity(name=f"{ip}:{p.strip()}", label="Indicator", reasoning="Extracted Socket"))
        return new_entities
    return [entity]

def refine_graph_data(llm_data: GraphExtraction, regex_entities: List[Entity]) -> GraphExtraction:
    """LLM ë°ì´í„° + Regex ë°ì´í„° ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
    
    # 1. ë³‘í•© (LLM ìš°ì„ )
    all_entities = llm_data.entities + regex_entities
    
    # 2. ì¤‘ë³µ ì œê±° (ì´ë¦„ ê¸°ì¤€)
    unique_map = {}
    for ent in all_entities:
        clean_name = clean_indicator(ent.name)
        # ì´ë¯¸ ì¡´ìž¬í•˜ëŠ”ë° í˜„ìž¬ ê²ƒì´ Regexë¼ë©´ ìŠ¤í‚µ (LLMì˜ ë¼ë²¨/ì„¤ëª…ì´ ë” ì •í™•í•  ìˆ˜ ìžˆìŒ)
        if clean_name in unique_map and "Regex" in ent.reasoning:
            continue
        ent.name = clean_name
        unique_map[clean_name] = ent
    
    final_entities = []
    # 3. ë¶„ë¦¬ (Split Composite)
    for ent in unique_map.values():
        final_entities.extend(split_composite_indicator(ent))
        
    # 4. ê³ ì•„ ë…¸ë“œ ì—°ê²° (Orphan Linking)
    # LLMì´ Incidentë¥¼ ì°¾ì•˜ë‹¤ë©´, Regexë¡œ ì°¾ì€ ê³ ì•„ IOCë“¤ë„ ê±°ê¸°ì— ì—°ê²°í•´ì¤€ë‹¤.
    incidents = [e for e in final_entities if e.label == "Incident"]
    main_incident_name = incidents[0].name if incidents else "Detected Incident"
    
    # ê´€ê³„ ì—…ë°ì´íŠ¸
    final_rels = llm_data.relationships[:] # ë³µì‚¬
    existing_rel_targets = {r.target for r in final_rels}
    
    for ent in final_entities:
        # ê´€ê³„ê°€ ì—†ëŠ” Indicator/MalwareëŠ” ë©”ì¸ ì‚¬ê±´ì— ì—°ê²°
        if ent.label in ["Indicator", "Malware"] and ent.name not in existing_rel_targets:
            # ì‚¬ê±´ -> ì§€í‘œ ì—°ê²°
            final_rels.append(Relationship(
                source=main_incident_name,
                target=ent.name,
                type="HAS_INDICATOR" if ent.label == "Indicator" else "USES_MALWARE"
            ))

    return GraphExtraction(entities=final_entities, relationships=final_rels)

# ==============================================================================
# 4. Grounding Logic
# ==============================================================================
def calculate_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def normalize_entity(entity: Entity) -> Entity:
    clean_name = entity.name
    
    if entity.label in ["Incident", "SecurityEntity"]:
        entity.normalized_name = clean_name
        entity.match_score = 1.0
        return entity

    query = f"""
    MATCH (n)
    WHERE toLower(n.name) CONTAINS toLower($name) 
       OR toLower($name) CONTAINS toLower(n.name)
    RETURN n.name as name, coalesce(n.id, elementId(n)) as id, labels(n) as labels
    LIMIT 10
    """
    
    try:
        results = graph_client.query(query, {"name": clean_name})
    except Exception:
        results = []
    
    best_match = None
    best_score = 0.0

    if results:
        for r in results:
            score = calculate_similarity(clean_name, r['name'])
            if clean_name.lower() == r['name'].lower(): score = 1.0
            
            valid_labels = [l for l in r['labels'] if l not in ['BaseNode', 'Resource', 'Entity']]
            primary_label = valid_labels[0] if valid_labels else r['labels'][0]

            if score > best_score:
                best_score = score
                best_match = {"name": r['name'], "id": r['id'], "label": primary_label}

    # [ìˆ˜ì •] ìž„ê³„ê°’ ìƒí–¥ (0.6 -> 0.8) : NetCat <-> Net ì˜¤íƒì§€ ë°©ì§€
    if best_match and best_score >= 0.8: 
        entity.normalized_name = best_match['name']
        entity.existing_id = str(best_match['id'])
        entity.db_label = best_match['label']
        entity.match_score = best_score
    else:
        entity.normalized_name = clean_name
        entity.db_label = entity.label
        entity.match_score = best_score if best_match else 0.0
        
    return entity

# ==============================================================================
# 5. ì‹¤í–‰
# ==============================================================================
def run_interactive_test():
    extractor = get_extractor()
    
    print("\n" + "="*60)
    print("ðŸ“ Paste your threat report text below. (Ctrl+D to submit)")
    print("="*60)
    
    try:
        lines = sys.stdin.readlines()
    except EOFError:
        pass
    text_input = "".join(lines).strip()
    if not text_input: return

    print(f"\nðŸš€ [Step 1] Hybrid Extraction (LLM + Regex)...")
    
    # 1. LLM Extraction
    llm_result = extractor.invoke(text_input)
    print(f"   -> LLM found: {len(llm_result.entities)} entities.")
    
    # 2. Regex Extraction
    regex_entities = extract_iocs_regex(text_input)
    print(f"   -> Regex found: {len(regex_entities)} potential IOCs.")
    
    # 3. Refinement & Merge
    result = refine_graph_data(llm_result, regex_entities)
    valid_entities = [e for e in result.entities if e.label != "SecurityEntity"]
    print(f"   -> Merged Total: {len(valid_entities)} entities.")

    print("\nðŸš€ [Step 2] Grounding with Neo4j...")
    
    normalized_entities = []
    
    for ent in valid_entities:
        norm_ent = normalize_entity(ent)
        normalized_entities.append(norm_ent)

    # ì¶œë ¥
    print("\n" + "="*80)
    print(f"{'TYPE':<15} | {'NAME':<35} | {'STATUS':<10} | {'SCORE'}")
    print("-" * 80)
    
    for ent in normalized_entities:
        status = "EXISTING" if ent.existing_id else "NEW"
        label_display = ent.db_label if ent.db_label else ent.label
        # ì´ë¦„ì´ ë„ˆë¬´ ê¸¸ë©´ ìžë¦„
        display_name = (ent.normalized_name[:32] + '..') if len(ent.normalized_name) > 32 else ent.normalized_name
        print(f"{label_display:<15} | {display_name:<35} | {status:<10} | {ent.match_score:.2f}")

    print("\n[Edges (Sample)]")
    count = 0
    for rel in result.relationships:
        if count > 20: 
            print("... (more edges hidden)")
            break
        print(f"  ({rel.source}) --[{rel.type}]--> ({rel.target})")
        count += 1

if __name__ == "__main__":
    run_interactive_test()