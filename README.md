# üõ°Ô∏è Cyber Ontology INtelligence (COIN)

**A Next-Gen Threat Intelligence Platform powered by Knowledge Graphs and Autonomous AI Agents.**

COIN is a platform that transforms fragmented and unstructured cyber threat data into a machine-readable **Knowledge Graph (Neo4j)**. It bridges the gap between raw text reports and actionable intelligence by using **LLMs (via LangGraph)** to extract entities, map attack flows, and provide an autonomous reasoning agent for complex security queries.

---

## ‚ú® Key Features

1.  **Deep Analysis**: Performs in-depth profiling of specific entities like Threat Groups, Malware, and Vulnerabilities, generating AI-powered reports based on graph data.
2.  **Correlation**: Traces hidden connections between disparate IoCs (IPs, Hashes, URLs) to identify the threat actors or campaigns behind them.
3.  **Graph Analysis**: Visually explores attack scenarios (Incidents) step-by-step, tracking the kill chain from initial access to final impact.
4.  **Ontology Extractor**: Automatically structures unstructured CTI reports (text) into a formal ontology (Incident -> Step -> Entity) using LLMs and Regex, ready for graph ingestion.
5.  **Smart Agent (Chatbot)**: An AI analyst that understands natural language, queries the knowledge graph in real-time, and provides context-aware answers about complex threat relationships.

---

## üèóÔ∏è Architecture

The system follows a modular architecture separating the Data, Backend, and Application layers. All components are designed to work together to provide a comprehensive intelligence lifecycle.

```mermaid
graph TD
    subgraph "Data Sources"
        direction LR
        S1[MITRE ATT&CK]
        S2[CISA KEV]
        S3[URLhaus]
        S4["Unstructured Reports<br/>(PDF, TXT, Blogs)"]
    end

    subgraph "Data Processing Layer"
        direction TB
        ETL["ETL Scripts<br/>(scripts/etl/*.py)"]
        Processor["Intelligence Processor<br/>(src/services/intelligence_processor.py)"]
        LLM[(LLM<br/>Ollama/OpenAI)]
        
        S1 & S2 & S3 --> ETL
        S4 --> Processor
        Processor -- "Extraction & Structuring" --> LLM
    end

    subgraph "Knowledge Base"
        Neo4j[(Neo4j Graph DB)]
    end

    ETL --> Neo4j
    Processor -- "Ingests Structured Graph" --> Neo4j

    subgraph "Core Services (Backend)"
        direction TB
        GraphClient["Graph Client<br/>(src/core/graph_client.py)"]
        Services["Business Logic<br/>(src/services/*)"]
        Tools["Agent Tools<br/>(src/tools/neo4j.py)"]
        Agent["Smart Agent<br/>(src/services/agent.py)"]
    end
    
    Neo4j <--> GraphClient
    GraphClient <--> Services
    Services --> Agent
    Tools <--> Agent
    
    subgraph "Application Layer"
        direction LR
        UI["Streamlit UI<br/>(apps/ui)"]
        CLI["CLI App<br/>(apps/cli)"]
        MCP["MCP Server<br/>(apps/mcp)"]
    end

    Services --> UI
    Services --> CLI
    Agent --> UI
    Agent --> CLI
    Tools --> MCP

    style Neo4j fill:#0088CC,stroke:#00659c,color:white
    style LLM fill:#FF6F61,stroke:#D95C50,color:white
```

---

## üìÇ Directory Structure

```text
cyber-ontology/
‚îú‚îÄ‚îÄ apps/                 # Application Entry Points
‚îÇ   ‚îú‚îÄ‚îÄ cli/              # Command Line Interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ mcp/              # MCP Tool Server
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server_neo4j.py
‚îÇ   ‚îî‚îÄ‚îÄ ui/               # Streamlit Web Dashboard
‚îÇ       ‚îú‚îÄ‚îÄ Home.py       # System Dashboard & Navigation
‚îÇ       ‚îî‚îÄ‚îÄ pages/        # UI Pages for each feature
‚îÇ           ‚îú‚îÄ‚îÄ 1_Deep_Analysis.py
‚îÇ           ‚îú‚îÄ‚îÄ 2_Correlation.py
‚îÇ           ‚îú‚îÄ‚îÄ 3_Scenario_Explorer.py
‚îÇ           ‚îú‚îÄ‚îÄ 4_Intelligence_Processing.py
‚îÇ           ‚îî‚îÄ‚îÄ 5_Smart_Agent.py
‚îú‚îÄ‚îÄ src/                  # Core Business Logic
‚îÇ   ‚îú‚îÄ‚îÄ core/             # Config, DB Client, Pydantic Schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îÇ   ‚îú‚îÄ‚îÄ services/         # Business Logic Services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correlation.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ intelligence_processor.py
‚îÇ   ‚îî‚îÄ‚îÄ tools/            # Modular LangChain Tools for the Agent
‚îÇ       ‚îî‚îÄ‚îÄ neo4j.py
‚îú‚îÄ‚îÄ data/                 # Data Storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Original downloaded data
‚îÇ   ‚îú‚îÄ‚îÄ seed/             # Seed data for scenario generation (Organizations)
‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Preprocessed data for Neo4j import
‚îÇ   ‚îî‚îÄ‚îÄ generated/        # AI-generated incident data
‚îú‚îÄ‚îÄ scripts/              # Automation & Setup Scripts
‚îÇ   ‚îú‚îÄ‚îÄ etl/              # ETL pipelines (Raw -> Processed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_mitre.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ process_incidents.py
‚îÇ   ‚îî‚îÄ‚îÄ setup/            # Initial setup scripts
‚îÇ       ‚îú‚îÄ‚îÄ bootstrap.sh
‚îÇ       ‚îú‚îÄ‚îÄ generate_incidents.py
‚îÇ       ‚îî‚îÄ‚îÄ init_db.py
‚îú‚îÄ‚îÄ .env.example          # Environment variable template
‚îú‚îÄ‚îÄ docker-compose.yml    # Docker configuration for Neo4j
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ requirements.txt      # Python Dependencies
```

---

## üöÄ Getting Started

### 1. Prerequisites

*   **Python 3.10+**
*   **Docker** and **Docker Compose**
*   **Ollama** (for local LLM) or an **OpenAI API Key**

### 2. Environment Setup

Clone the repository and set up the Python virtual environment.

```bash
git clone https://github.com/your-username/cyber-ontology.git
cd cyber-ontology
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Configuration

Copy the example `.env` file and update it with your Neo4j and LLM credentials.

```bash
cp .env.example .env
```

Then, edit `.env` with your details:
```ini
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_secret_password
LLM_PROVIDER=ollama  # or openai
OLLAMA_MODEL=llama3.1 # if using ollama
OPENAI_API_KEY=sk-... # if using openai
```

### 4. Launch Services

Start the Neo4j database container in the background.

```bash
docker-compose up -d
```

> **Note:** If you are using a local LLM, ensure your Ollama service is running (`ollama serve`).

### 5. Data Pipeline Execution

Run the master script to download datasets, preprocess them, and populate the Neo4j database. This script will guide you through the incident data setup.

```bash
# Grant execution permission
chmod +x scripts/setup_pipeline.sh

# Run the full pipeline
./scripts/setup_pipeline.sh
```

---

## üñ•Ô∏è Usage

### GUI (Recommended)

The primary way to use COIN is through the Streamlit web dashboard, which provides access to all features.

```bash
streamlit run apps/ui/Home.py
```

1.  **Ingest Data**: Go to `4_Intelligence_Processing`, paste a raw CTI report, and click **Analyze**. Review the extracted graph and click **Ingest into Neo4j**.
2.  **Analyze Context**: Use the `3_Scenario_Explorer` to see how the attack unfolded step-by-step.
3.  **Chat with Agent**: Open `5_Smart_Agent` and ask: *"CVE-2025-55182Ïóê ÎåÄÌï¥ ÏïåÎ†§Ï§ò. Ïñ¥Îñ§ ÏÇ¨Í±¥Ïù¥Îûë Ïó∞Í¥ÄÎêòÏñ¥ ÏûàÏñ¥?"*

### CLI (Limited Features)

A command-line interface is also available for core analysis functions.

```bash
python apps/cli/main.py
```
The CLI supports the following features:
*   **Deep Analysis** (Feature 1)
*   **Correlation** (Feature 2)
*   **Smart Agent** (Feature 5)

---

## üõ°Ô∏è Roadmap

### Completed
* [x] Migration to Neo4j Property Graph Model
* [x] Unstructured CTI Report Processor (Text-to-Graph)
* [x] Context-Aware Autonomous Agent (Incident-Step-Entity Tracing)
* [x] Entity Grounding & Normalization Logic

### Planned
* [ ] Live Data Integration via Multi-Modal MCP Server

---

**¬© 2026 Cyber Ontology Intelligence Project. Powered by Neo4j & LangGraph.**
