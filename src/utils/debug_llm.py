# debug_llm.py
# LLM ì„¤ì • ë° ì—°ê²° ìƒíƒœë¥¼ ì§„ë‹¨í•˜ëŠ” ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
# ì‚¬ìš©ë²•: í„°ë¯¸ë„ì—ì„œ `python src/utils/debug_llm.py` ì‹¤í–‰


import sys
import os
import requests

# ------------------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •] í˜„ì¬ ìœ„ì¹˜(src/core/utils)ì—ì„œ ë£¨íŠ¸(../../..)ë¥¼ ì°¾ì•„ Pathì— ì¶”ê°€
# ------------------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../../"))

if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from src.core.config import settings
    from src.core.llm import chat
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"   í˜„ì¬ ì¸ì‹ëœ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    print("   í”„ë¡œì íŠ¸ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)


def run_diagnostics():
    print(f"ğŸ” [LLM ì§„ë‹¨ ì‹œì‘]")
    print(f"--------------------------------------------------")
    print(f"1. ì„¤ì • í™•ì¸")
    print(f"   - Provider: {settings.LLM_PROVIDER}")
    
    if settings.LLM_PROVIDER == "ollama":
        print(f"   - Base URL: {settings.OLLAMA_BASE_URL}")
        print(f"   - Model:    {settings.OLLAMA_MODEL}")
        
        # 2. Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
        print(f"\n2. Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ ({settings.OLLAMA_BASE_URL})")
        try:
            r = requests.get(settings.OLLAMA_BASE_URL, timeout=5)
            if r.status_code == 200:
                print(f"   âœ… ì„œë²„ ì—°ê²° ì„±ê³µ! (Ollama is running)")
            else:
                print(f"   âŒ ì„œë²„ ì‘ë‹µ ì´ìƒ: {r.status_code}")
        except Exception as e:
            print(f"   âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print(f"   ğŸ‘‰ 'ollama serve'ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return

        # 3. ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        print(f"\n3. ëª¨ë¸ í™•ì¸ ('{settings.OLLAMA_MODEL}')")
        try:
            r = requests.get(f"{settings.OLLAMA_BASE_URL.rstrip('/')}/api/tags", timeout=5)
            models = [m['name'] for m in r.json().get('models', [])]
            
            # íƒœê·¸ ë§¤ì¹­ (latest íƒœê·¸ ì²˜ë¦¬ í¬í•¨)
            if any(settings.OLLAMA_MODEL in m for m in models):
                print(f"   âœ… ëª¨ë¸ ë°œê²¬ë¨: {settings.OLLAMA_MODEL}")
            else:
                print(f"   âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ!")
                print(f"   ğŸ‘‰ í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸: {models}")
                print(f"   ğŸ‘‰ í•´ê²°ì±…: í„°ë¯¸ë„ì— 'ollama pull {settings.OLLAMA_MODEL}' ì…ë ¥")
                return
        except Exception as e:
            print(f"   âš ï¸ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    elif settings.LLM_PROVIDER == "openai":
        print(f"   - Model: {settings.OPENAI_MODEL}")
        if not settings.OPENAI_API_KEY:
            print("   âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        print("   âœ… API Key ì„¤ì •ë¨")

    # 4. ì‹¤ì œ ìƒì„± í…ŒìŠ¤íŠ¸
    print(f"\n4. ì‹¤ì œ ìƒì„± í…ŒìŠ¤íŠ¸ (Hello World)")
    messages = [{"role": "user", "content": "Say 'Connection Successful' in Korean."}]
    
    try:
        print("   â³ ìš”ì²­ ì „ì†¡ ì¤‘...")
        response = chat(messages)
        if response:
            print(f"   âœ… ì‘ë‹µ ìˆ˜ì‹  ì„±ê³µ:")
            print(f"   >> {response}")
        else:
            print(f"   âŒ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (src/core/llm.py ë‚´ë¶€ ì˜¤ë¥˜ ë¡œê·¸ í™•ì¸ í•„ìš”)")
    except Exception as e:
        print(f"   âŒ ìƒì„± ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    run_diagnostics()